{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attempt 4"
      ],
      "metadata": {
        "id": "IlRhX0EfXtI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Audio Transcription with Speaker Diarization for Google Colab\n",
        "# Designed for free tier constraints with one-file-at-a-time processing\n",
        "\n",
        "# ============================================\n",
        "# BLOCK 1: Environment Reset and GPU Check\n",
        "# ============================================\n",
        "# Run this first to ensure clean environment\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check current environment\n",
        "print(\"Checking environment...\")\n",
        "!nvidia-smi -L\n",
        "!python --version\n",
        "\n",
        "# Clean any corrupted installations\n",
        "!rm -rf /usr/local/lib/python3.*/dist-packages/~orch 2>/dev/null || true\n",
        "!pip cache purge -q\n",
        "\n",
        "print(\"\\n✓ Environment cleaned. Proceed to Block 2.\")"
      ],
      "metadata": {
        "id": "fy55ENzlX11_",
        "outputId": "59e80fcc-df18-4b4d-c61f-a63bd4289dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking environment...\n",
            "GPU 0: Tesla T4 (UUID: GPU-3f5ef6a4-1e4f-ecac-a352-ec20ecfd4813)\n",
            "Python 3.11.13\n",
            "\n",
            "✓ Environment cleaned. Proceed to Block 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 2: Strategic Dependency Installation\n",
        "# ============================================\n",
        "# CRITICAL: Run this in exact order\n",
        "print(\"Installing dependencies in correct order...\")\n",
        "\n",
        "# Step 1: Force NumPy 1.x to avoid compatibility issues\n",
        "!pip uninstall -y numpy -q\n",
        "!pip install numpy==1.24.3 -q\n",
        "\n",
        "# Step 2: Install PyTorch with specific CUDA version\n",
        "!pip install torch==2.1.2+cu118 torchaudio==2.1.2+cu118 --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "\n",
        "# Step 3: Install critical dependencies with version pins\n",
        "!pip install transformers==4.36.2 -q\n",
        "!pip install faster-whisper==1.0.3 -q\n",
        "!pip install ctranslate2==4.4.0 -q  # Critical for Colab\n",
        "\n",
        "# Step 4: Install audio processing libraries\n",
        "!pip install pydub==0.25.1 -q\n",
        "!pip install librosa==0.10.1 -q\n",
        "\n",
        "# Step 5: Install pyannote.audio\n",
        "!pip install pyannote.audio==3.1.1 -q\n",
        "\n",
        "# Step 6: Install WhisperX without dependencies\n",
        "!pip install --no-deps git+https://github.com/m-bain/whisperx.git@v3.1.1 -q\n",
        "\n",
        "# Step 7: Install remaining WhisperX requirements\n",
        "!pip install pandas==2.0.3 -q  # Compatible with Colab\n",
        "!pip install nltk>=3.8 -q\n",
        "!pip install ffmpeg-python==0.2.0 -q\n",
        "\n",
        "print(\"\\n✓ Dependencies installed. Restart runtime if you see errors.\")\n",
        "print(\"After restart, run from Block 3 onwards.\")"
      ],
      "metadata": {
        "id": "fYgPyUmGX7dn",
        "outputId": "091c2c37-50e7-449b-c672-f5a05c7a8675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies in correct order...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "whisperx 3.1.1 requires setuptools==65.6.3, but you have setuptools 75.2.0 which is incompatible.\n",
            "whisperx 3.1.1 requires torch==2.0.0, but you have torch 2.1.2+cu118 which is incompatible.\n",
            "whisperx 3.1.1 requires torchaudio==2.0.1, but you have torchaudio 2.1.2+cu118 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu118 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "✓ Dependencies installed. Restart runtime if you see errors.\n",
            "After restart, run from Block 3 onwards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 3: Import and Verify Installation\n",
        "# ============================================\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Verify installations\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Test critical imports\n",
        "try:\n",
        "    import whisperx\n",
        "    print(\"✓ WhisperX imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ WhisperX import error: {e}\")\n",
        "    print(\"Please restart runtime and run from Block 3\")\n",
        "\n",
        "try:\n",
        "    from pyannote.audio import Pipeline\n",
        "    print(\"✓ pyannote.audio imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ pyannote.audio import error: {e}\")"
      ],
      "metadata": {
        "id": "AXOYg-BjX-_q",
        "outputId": "48356e19-762b-4884-d79d-9b845f5c5b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.2+cu118\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n",
            "❌ WhisperX import error: module 'numpy' has no attribute 'dtypes'\n",
            "Please restart runtime and run from Block 3\n",
            "❌ pyannote.audio import error: module 'numpy' has no attribute 'dtypes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 4: Mount Drive and Setup Paths\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configure paths - adjust these to your structure\n",
        "INPUT_PATH = \"/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/PRUT-Transcriptions/Transcripts\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# List files\n",
        "import glob\n",
        "mp4_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.mp4\")))\n",
        "mp3_files = sorted(glob.glob(os.path.join(INPUT_PATH, \"*.mp3\")))\n",
        "all_files = mp4_files + mp3_files\n",
        "\n",
        "print(f\"\\nFound {len(all_files)} audio files:\")\n",
        "print(f\"  - {len(mp4_files)} MP4 files\")\n",
        "print(f\"  - {len(mp3_files)} MP3 files\")\n",
        "\n",
        "# Check completed files\n",
        "completed = [f for f in os.listdir(OUTPUT_PATH) if f.endswith('_transcript.md')]\n",
        "completed_bases = [f.replace('_transcript.md', '') for f in completed]\n",
        "\n",
        "remaining = []\n",
        "for f in all_files:\n",
        "    base = os.path.splitext(os.path.basename(f))[0]\n",
        "    if base not in completed_bases:\n",
        "        remaining.append(f)\n",
        "\n",
        "print(f\"\\nProgress:\")\n",
        "print(f\"  - Completed: {len(completed)}\")\n",
        "print(f\"  - Remaining: {len(remaining)}\")"
      ],
      "metadata": {
        "id": "m53avOK3YC-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 5: Configure HuggingFace Token\n",
        "# ============================================\n",
        "# Get HuggingFace token\n",
        "from google.colab import userdata\n",
        "\n",
        "# Try to get from Colab secrets first\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"✓ Found HF token in Colab secrets\")\n",
        "except:\n",
        "    HF_TOKEN = None\n",
        "\n",
        "# If not in secrets, ask for it\n",
        "if not HF_TOKEN:\n",
        "    print(\"\\nHuggingFace token required for speaker diarization.\")\n",
        "    print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"Accept conditions at: https://huggingface.co/pyannote/speaker-diarization-3.1\")\n",
        "    HF_TOKEN = input(\"Enter your HuggingFace token: \")\n",
        "\n",
        "# Set as environment variable\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
      ],
      "metadata": {
        "id": "Gjz3Sja2YDpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 6: Audio Conversion Function\n",
        "# ============================================\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "\n",
        "def convert_to_wav(input_path, temp_dir=\"/content/temp_audio\"):\n",
        "    \"\"\"Convert MP4/MP3 to WAV for processing\"\"\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
        "    output_path = os.path.join(temp_dir, f\"{base_name}.wav\")\n",
        "\n",
        "    # Skip if already WAV\n",
        "    if input_path.lower().endswith('.wav'):\n",
        "        return input_path\n",
        "\n",
        "    # Check if already converted\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"  Using cached WAV: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    print(f\"  Converting to WAV: {os.path.basename(input_path)}\")\n",
        "\n",
        "    try:\n",
        "        # Use ffmpeg for robust conversion\n",
        "        cmd = [\n",
        "            'ffmpeg', '-i', input_path,\n",
        "            '-acodec', 'pcm_s16le',\n",
        "            '-ar', '16000',  # 16kHz sample rate\n",
        "            '-ac', '1',      # Mono\n",
        "            '-y',            # Overwrite\n",
        "            output_path\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True, capture_output=True)\n",
        "        print(f\"  ✓ Converted successfully\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Conversion failed: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "IspwTnKMYHvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 7: Memory-Efficient Processing Function\n",
        "# ============================================\n",
        "def process_single_file(audio_path, output_path, hf_token):\n",
        "    \"\"\"Process one file with transcription and diarization\"\"\"\n",
        "\n",
        "    base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "    transcript_path = os.path.join(output_path, f\"{base_name}_transcript.md\")\n",
        "\n",
        "    # Skip if already processed\n",
        "    if os.path.exists(transcript_path):\n",
        "        print(f\"✓ Already processed: {base_name}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🎯 Processing: {os.path.basename(audio_path)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Convert to WAV if needed\n",
        "        wav_path = convert_to_wav(audio_path)\n",
        "        if not wav_path:\n",
        "            return False\n",
        "\n",
        "        # Load audio\n",
        "        print(\"\\n1️⃣ Loading audio...\")\n",
        "        audio = whisperx.load_audio(wav_path)\n",
        "        duration = len(audio) / 16000  # 16kHz sample rate\n",
        "        print(f\"  Duration: {duration:.1f} seconds\")\n",
        "\n",
        "        # Initialize WhisperX model\n",
        "        print(\"\\n2️⃣ Loading transcription model...\")\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        compute_type = \"int8\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "        model = whisperx.load_model(\n",
        "            \"large-v2\",  # Using v2 for stability\n",
        "            device,\n",
        "            compute_type=compute_type,\n",
        "            language=\"en\"\n",
        "        )\n",
        "\n",
        "        # Transcribe with small batch size\n",
        "        print(\"\\n3️⃣ Transcribing audio...\")\n",
        "        result = model.transcribe(\n",
        "            audio,\n",
        "            batch_size=4,  # Small batch for memory\n",
        "            language=\"en\",\n",
        "            suppress_tokens=[-1],  # Keep all tokens including fillers\n",
        "            condition_on_previous_text=True,\n",
        "            temperature=0,\n",
        "            compression_ratio_threshold=2.4,\n",
        "            logprob_threshold=-1.0,\n",
        "            no_speech_threshold=0.6\n",
        "        )\n",
        "\n",
        "        # Free transcription model memory\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Align whisper output\n",
        "        print(\"\\n4️⃣ Aligning timestamps...\")\n",
        "        model_a, metadata = whisperx.load_align_model(\n",
        "            language_code=\"en\",\n",
        "            device=device\n",
        "        )\n",
        "        result = whisperx.align(\n",
        "            result[\"segments\"],\n",
        "            model_a,\n",
        "            metadata,\n",
        "            audio,\n",
        "            device,\n",
        "            return_char_alignments=False\n",
        "        )\n",
        "\n",
        "        # Free alignment model\n",
        "        del model_a, metadata\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Speaker diarization\n",
        "        print(\"\\n5️⃣ Identifying speakers...\")\n",
        "        diarize_model = whisperx.DiarizationPipeline(\n",
        "            use_auth_token=hf_token,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Run diarization with conservative parameters\n",
        "        diarize_segments = diarize_model(\n",
        "            audio,\n",
        "            min_speakers=2,\n",
        "            max_speakers=10\n",
        "        )\n",
        "\n",
        "        # Assign speakers to words\n",
        "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "        # Free diarization model\n",
        "        del diarize_model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Save transcript\n",
        "        print(\"\\n6️⃣ Saving transcript...\")\n",
        "        with open(transcript_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Transcript: {base_name}\\n\\n\")\n",
        "            f.write(f\"**Date processed**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"**Duration**: {duration:.1f} seconds\\n\\n\")\n",
        "            f.write(\"---\\n\\n\")\n",
        "\n",
        "            current_speaker = None\n",
        "            for segment in result[\"segments\"]:\n",
        "                speaker = segment.get('speaker', 'UNKNOWN')\n",
        "\n",
        "                # New speaker section\n",
        "                if speaker != current_speaker:\n",
        "                    f.write(f\"\\n## {speaker}\\n\\n\")\n",
        "                    current_speaker = speaker\n",
        "\n",
        "                # Write segment with timestamp\n",
        "                start = segment['start']\n",
        "                end = segment['end']\n",
        "                text = segment['text'].strip()\n",
        "\n",
        "                # Keep all text including fillers\n",
        "                f.write(f\"[{start:.2f}s - {end:.2f}s] {text}\\n\\n\")\n",
        "\n",
        "        print(f\"\\n✅ Successfully saved: {transcript_path}\")\n",
        "\n",
        "        # Clean up temporary WAV if created\n",
        "        if wav_path != audio_path and os.path.exists(wav_path):\n",
        "            os.remove(wav_path)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error processing {base_name}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Emergency memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return False"
      ],
      "metadata": {
        "id": "MqTDZSfHYKJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 8: Process Next File (Run Repeatedly)\n",
        "# ============================================\n",
        "# This block processes ONE file at a time\n",
        "# Run it repeatedly until all files are done\n",
        "\n",
        "if remaining:\n",
        "    next_file = remaining[0]\n",
        "    print(f\"\\n🔄 Processing file {len(completed) + 1} of {len(all_files)}\")\n",
        "    print(f\"File: {os.path.basename(next_file)}\")\n",
        "\n",
        "    success = process_single_file(next_file, OUTPUT_PATH, HF_TOKEN)\n",
        "\n",
        "    if success:\n",
        "        # Update progress\n",
        "        completed = [f for f in os.listdir(OUTPUT_PATH) if f.endswith('_transcript.md')]\n",
        "        remaining = remaining[1:]\n",
        "\n",
        "        print(f\"\\n📊 Progress: {len(completed)}/{len(all_files)} completed\")\n",
        "        print(f\"⏭️  {len(remaining)} files remaining\")\n",
        "\n",
        "        if remaining:\n",
        "            print(\"\\n🔄 Run this cell again to process the next file\")\n",
        "        else:\n",
        "            print(\"\\n🎉 All files processed!\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  File failed. You can:\")\n",
        "        print(\"1. Run this cell again to retry\")\n",
        "        print(\"2. Skip by removing it from 'remaining' list\")\n",
        "\n",
        "    # Always clear memory after processing\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "    print(\"🎉 All files have been processed!\")\n",
        "    print(f\"\\n📁 Transcripts saved in: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "2hIqyS1RYNVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 9: Verification and Summary\n",
        "# ============================================\n",
        "# Run this to see processing summary\n",
        "\n",
        "print(\"📊 Processing Summary\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# List all transcripts\n",
        "transcripts = sorted([f for f in os.listdir(OUTPUT_PATH) if f.endswith('_transcript.md')])\n",
        "\n",
        "print(f\"\\nTotal transcripts: {len(transcripts)}\")\n",
        "print(\"\\nCompleted files:\")\n",
        "for t in transcripts:\n",
        "    size = os.path.getsize(os.path.join(OUTPUT_PATH, t)) / 1024\n",
        "    print(f\"  ✓ {t} ({size:.1f} KB)\")\n",
        "\n",
        "# Check for any missing files\n",
        "all_bases = [os.path.splitext(os.path.basename(f))[0] for f in all_files]\n",
        "completed_bases = [f.replace('_transcript.md', '') for f in transcripts]\n",
        "missing = [b for b in all_bases if b not in completed_bases]\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠️  Missing transcripts for:\")\n",
        "    for m in missing:\n",
        "        print(f\"  - {m}\")\n",
        "else:\n",
        "    print(\"\\n✅ All files successfully transcribed!\")"
      ],
      "metadata": {
        "id": "UTnUlPJ9YQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# BLOCK 10: Emergency Cleanup (If Needed)\n",
        "# ============================================\n",
        "# Run this if you encounter memory errors\n",
        "\n",
        "print(\"🧹 Performing emergency cleanup...\")\n",
        "\n",
        "# Clear all GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear temporary files\n",
        "temp_dir = \"/content/temp_audio\"\n",
        "if os.path.exists(temp_dir):\n",
        "    import shutil\n",
        "    shutil.rmtree(temp_dir)\n",
        "    print(f\"✓ Cleared temporary audio files\")\n",
        "\n",
        "# Show memory status\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nGPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB used\")\n",
        "    print(f\"GPU Memory: {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "\n",
        "print(\"\\n✓ Cleanup complete. You can continue processing.\")\n",
        "Improve\n",
        "Explain\n"
      ],
      "metadata": {
        "id": "x2zayCv4YQvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attempt 3"
      ],
      "metadata": {
        "id": "0STviUvtXotO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfTwdZOrR7XM",
        "outputId": "41300bf0-5903-4adc-b97e-f252c545483d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping whisperx as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pyannote.audio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mWed Jun 11 05:18:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Clean environment first\n",
        "!pip uninstall -y torch torchvision torchaudio transformers whisperx pyannote.audio pandas -q\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~orch\n",
        "!pip install pandas==2.2.2 -q  # Colab requirement\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch first with specific CUDA version for Colab\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install core dependencies with specific versions\n",
        "!pip install numpy>=1.26.4,<2.1\n",
        "!pip install transformers==4.44.2\n",
        "!pip install ctranslate2==4.4.0  # Critical: Colab requires 4.4.0, not 4.5.0+\n",
        "\n",
        "# Install pyannote.audio with NumPy 2.0 support\n",
        "!pip install pyannote.audio==3.3.2\n",
        "\n",
        "# Install WhisperX\n",
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "\n",
        "# Handle ONNX runtime conflicts\n",
        "!pip uninstall -y onnxruntime onnxruntime-gpu\n",
        "!pip install onnxruntime-gpu==1.16.3"
      ],
      "metadata": {
        "id": "N8hYSpu-dV4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch first with specific version\n",
        "!pip install torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "\n",
        "# Install whisperx dependencies separately\n",
        "!pip install faster-whisper==1.0.3 -q\n",
        "!pip install pyannote.audio==3.1.1 -q\n",
        "\n",
        "# Install whisperx without dependencies to avoid conflicts\n",
        "!pip install --no-deps git+https://github.com/m-bain/whisperx.git@v3.1.1 -q\n",
        "\n",
        "# Install remaining whisperx requirements manually\n",
        "!pip install nltk>=3.8 -q\n",
        "!pip install ffmpeg-python==0.2.0 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASFBcN5_VxQa",
        "outputId": "a0aa7876-eab5-4519-8540-261e4e29aa29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m570.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires transformers, which is not installed.\n",
            "timm 1.0.15 requires torchvision, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "whisperx 3.1.1 requires transformers, which is not installed.\n",
            "whisperx 3.1.1 requires setuptools==65.6.3, but you have setuptools 75.2.0 which is incompatible.\n",
            "whisperx 3.1.1 requires torch==2.0.0, but you have torch 2.1.2+cu118 which is incompatible.\n",
            "whisperx 3.1.1 requires torchaudio==2.0.1, but you have torchaudio 2.1.2+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Configuration for memory efficiency\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 4  # Reduced from default 16\n",
        "compute_type = \"int8\"  # Instead of \"float16\"\n",
        "\n",
        "# Load and process transcription first\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=\"int8\")   # why not v3?\n",
        "result = model.transcribe(audio, batch_size=4)\n",
        "\n",
        "# Critical: Clear model from memory before diarization\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Then load diarization pipeline\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)"
      ],
      "metadata": {
        "id": "tmjopLohdZcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Test whisperx import\n",
        "try:\n",
        "    import whisperx\n",
        "    print(\"✓ WhisperX imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ WhisperX import error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFoP8rc-V2cD",
        "outputId": "1021f260-bdd3-42d0-b455-ddf0431ebfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-2057806252>\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.2+cu118\n",
            "CUDA available: True\n",
            "❌ WhisperX import error: No module named 'transformers'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 2: Install Correct Versions"
      ],
      "metadata": {
        "id": "zkLbWrCWVNnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install working combination\n",
        "!pip install torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!pip install faster-whisper==1.0.3 -q\n",
        "!pip install pyannote.audio==3.1.1 -q\n",
        "!pip install pydub -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnCN4LpsSAK4",
        "outputId": "8e5054b5-5c14-47b7-d3e9-f402adee64c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m706.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 3: Mount Drive and Setup"
      ],
      "metadata": {
        "id": "ZclyI6zJVD_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup paths - adjust to your actual path\n",
        "# Setup paths - adjust to your actual path\n",
        "INPUT_PATH = \"/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/PRUT-Transcriptions/Transcripts\"\n",
        "\n",
        "# Create output directory if needed\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Get list of MP4 files\n",
        "mp4_files = sorted([f for f in os.listdir(INPUT_PATH) if f.endswith('.mp4')])\n",
        "print(f\"Found {len(mp4_files)} MP4 files\")\n",
        "\n",
        "# Check which are already done\n",
        "completed = [f.replace('_transcript.md', '.mp4') for f in os.listdir(OUTPUT_PATH) if f.endswith('_transcript.md')]\n",
        "remaining = [f for f in mp4_files if f not in completed]\n",
        "\n",
        "print(f\"Already completed: {len(completed)}\")\n",
        "print(f\"Remaining to process: {len(remaining)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gFCdkGbSLWB",
        "outputId": "ac7f6e35-fdb1-48c2-c722-1820eeb96c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 7 MP4 files\n",
            "Already completed: 0\n",
            "Remaining to process: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n4n91uaoTAM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36bbdba-a1ce-4a8b-eadc-d5782ddcde0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 4: Setup WhisperX with Diarization"
      ],
      "metadata": {
        "id": "FYkrhOIJVJ_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your HuggingFace token\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')  # Add your token in Colab secrets\n",
        "\n",
        "# If no token in secrets, ask for it\n",
        "if not HF_TOKEN:\n",
        "    HF_TOKEN = input(\"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\")\n",
        "\n",
        "# Install WhisperX\n",
        "!pip install git+https://github.com/m-bain/whisperx.git -q\n",
        "\n",
        "import whisperx\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds8DsGyuSNKa",
        "outputId": "e492e16f-be34-4d33-915e-3a25cd88e95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m907.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whisperx (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.15 requires torchvision, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 5: Process One File Function"
      ],
      "metadata": {
        "id": "S2o1aCt3SXDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_file(mp4_file, input_path, output_path, hf_token):\n",
        "    \"\"\"Process a single audio file with WhisperX and speaker diarization\"\"\"\n",
        "\n",
        "    input_file = os.path.join(input_path, mp4_file)\n",
        "    base_name = mp4_file.replace('.mp4', '')\n",
        "    output_file = os.path.join(output_path, f\"{base_name}_transcript.md\")\n",
        "\n",
        "    # Skip if already processed\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"✓ Already processed: {mp4_file}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"\\n🎯 Processing: {mp4_file}\")\n",
        "\n",
        "    try:\n",
        "        # Load audio\n",
        "        audio = whisperx.load_audio(input_file)\n",
        "\n",
        "        # 1. Transcribe with Whisper\n",
        "        print(\"  → Transcribing...\")\n",
        "        model = whisperx.load_model(\"large-v2\", device, compute_type=\"float16\")\n",
        "        result = model.transcribe(audio, batch_size=16)\n",
        "\n",
        "        # 2. Align whisper output\n",
        "        print(\"  → Aligning...\")\n",
        "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "        # 3. Diarize with pyannote\n",
        "        print(\"  → Speaker diarization...\")\n",
        "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
        "        diarize_segments = diarize_model(audio, min_speakers=2, max_speakers=10)\n",
        "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "        # 4. Save as markdown\n",
        "        print(\"  → Saving transcript...\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Transcript: {mp4_file}\\n\\n\")\n",
        "\n",
        "            current_speaker = None\n",
        "            for segment in result[\"segments\"]:\n",
        "                speaker = segment.get('speaker', 'UNKNOWN')\n",
        "\n",
        "                # New speaker section\n",
        "                if speaker != current_speaker:\n",
        "                    f.write(f\"\\n## {speaker}\\n\\n\")\n",
        "                    current_speaker = speaker\n",
        "\n",
        "                # Write text with timestamp\n",
        "                start = segment['start']\n",
        "                end = segment['end']\n",
        "                text = segment['text'].strip()\n",
        "                f.write(f\"[{start:.2f}s - {end:.2f}s] {text}\\n\\n\")\n",
        "\n",
        "        print(f\"✓ Completed: {mp4_file}\")\n",
        "\n",
        "        # Clean up memory\n",
        "        del model, model_a, diarize_model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {mp4_file}: {str(e)}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "4BJTrcwhSVMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 6: Process Next File (Run Multiple Times)"
      ],
      "metadata": {
        "id": "R3nJjTUVU7eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process just ONE file at a time to avoid runtime crashes\n",
        "if remaining:\n",
        "    next_file = remaining[0]\n",
        "    print(f\"Processing next file: {next_file}\")\n",
        "\n",
        "    success = process_single_file(next_file, INPUT_PATH, OUTPUT_PATH, HF_TOKEN)\n",
        "\n",
        "    if success:\n",
        "        print(f\"\\n✅ Successfully processed {next_file}\")\n",
        "        print(f\"⏭️  {len(remaining)-1} files remaining\")\n",
        "        print(\"\\n🔄 Run this cell again to process the next file\")\n",
        "    else:\n",
        "        print(f\"\\n❌ Failed to process {next_file}\")\n",
        "        print(\"Fix the error and run again\")\n",
        "else:\n",
        "    print(\"🎉 All files have been processed!\")\n",
        "\n",
        "# Show progress\n",
        "completed = [f for f in os.listdir(OUTPUT_PATH) if f.endswith('_transcript.md')]\n",
        "print(f\"\\nProgress: {len(completed)}/{len(mp4_files)} files completed\")"
      ],
      "metadata": {
        "id": "OUUwyRX1SZxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fe7281-cc18-4e32-c7a6-e0a1b39c3228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing next file: Call Recording - 13Mar2025 1200 BPA.mp4\n",
            "\n",
            "🎯 Processing: Call Recording - 13Mar2025 1200 BPA.mp4\n",
            "  → Transcribing...\n",
            "❌ Error processing Call Recording - 13Mar2025 1200 BPA.mp4: module 'torch.utils._pytree' has no attribute 'register_pytree_node'\n",
            "\n",
            "❌ Failed to process Call Recording - 13Mar2025 1200 BPA.mp4\n",
            "Fix the error and run again\n",
            "\n",
            "Progress: 0/7 files completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 7: Alternative - Process Without HF Token"
      ],
      "metadata": {
        "id": "FScF3oT4U-oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you don't have a HuggingFace token, use this simpler version\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "def process_simple(mp4_file, input_path, output_path):\n",
        "    \"\"\"Simple transcription without speaker diarization\"\"\"\n",
        "\n",
        "    input_file = os.path.join(input_path, mp4_file)\n",
        "    base_name = mp4_file.replace('.mp4', '')\n",
        "    output_file = os.path.join(output_path, f\"{base_name}_transcript.md\")\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"✓ Already processed: {mp4_file}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"\\n🎯 Processing: {mp4_file}\")\n",
        "\n",
        "    try:\n",
        "        model = WhisperModel(\"large-v3\", device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "        segments, info = model.transcribe(\n",
        "            input_file,\n",
        "            language=\"en\",\n",
        "            word_timestamps=True,\n",
        "            vad_filter=True\n",
        "        )\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Transcript: {mp4_file}\\n\\n\")\n",
        "\n",
        "            for segment in segments:\n",
        "                f.write(f\"[{segment.start:.2f}s - {segment.end:.2f}s] {segment.text}\\n\\n\")\n",
        "\n",
        "        print(f\"✓ Completed: {mp4_file}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Use this if no HF token\n",
        "if remaining and not HF_TOKEN:\n",
        "    next_file = remaining[0]\n",
        "    process_simple(next_file, INPUT_PATH, OUTPUT_PATH)"
      ],
      "metadata": {
        "id": "lUHKYbtXSby3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "32yV2xXjU5XC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}