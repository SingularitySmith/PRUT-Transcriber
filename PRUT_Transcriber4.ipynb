{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH24Ino9ZC0YMuLlWNp/AH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SingularitySmith/PRUT-Transcriber/blob/main/PRUT_Transcriber4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WhisperX Ultra-Resilient Transcription System\n",
        "# Single-cell design with aggressive memory management\n",
        "\n",
        "\"\"\"\n",
        "CODE BLOCK 1: COMPLETE SYSTEM - RUN THIS SINGLE CELL REPEATEDLY\n",
        "This cell contains the entire system and can be run after crashes\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "VpZjKDKxjcbZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.1: CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "SOURCE_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/Transcripts'\n",
        "WAV_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/WAV_Cache'\n",
        "CHECKPOINT_FILE = '/content/drive/MyDrive/PRUT-Transcriptions/checkpoint.json'\n",
        "LOG_FILE = '/content/drive/MyDrive/PRUT-Transcriptions/processing.log'\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [OUTPUT_DIR, WAV_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "OAL39deKkTPN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.2: LOGGING SYSTEM\n",
        "# ============================================\n",
        "\n",
        "def log_message(message, level=\"INFO\"):\n",
        "    \"\"\"Log to both console and file\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    log_entry = f\"[{timestamp}] {level}: {message}\"\n",
        "    print(log_entry)\n",
        "\n",
        "    try:\n",
        "        with open(LOG_FILE, 'a') as f:\n",
        "            f.write(log_entry + \"\\n\")\n",
        "    except:\n",
        "        pass  # Don't fail if can't write log\n"
      ],
      "metadata": {
        "id": "ryoc3lg-kKnq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.3: CHECKPOINT MANAGEMENT\n",
        "# ============================================\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load progress from checkpoint file\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {\n",
        "        'processed_files': [],\n",
        "        'failed_files': {},\n",
        "        'current_mode': 'ultra_minimal',  # Start with absolute minimum\n",
        "        'model_loaded': None,\n",
        "        'last_update': None,\n",
        "        'session_count': 0\n",
        "    }\n",
        "\n",
        "def save_checkpoint(checkpoint):\n",
        "    \"\"\"Save progress to checkpoint file\"\"\"\n",
        "    checkpoint['last_update'] = datetime.now().isoformat()\n",
        "    checkpoint['session_count'] = checkpoint.get('session_count', 0) + 1\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "    log_message(f\"Checkpoint saved (session #{checkpoint['session_count']})\")\n"
      ],
      "metadata": {
        "id": "SKkkHffTkFxN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.4: PROCESSING MODES\n",
        "# ============================================\n",
        "\n",
        "PROCESSING_MODES = {\n",
        "    'ultra_minimal': {\n",
        "        'method': 'whisper_api',  # Use OpenAI Whisper API directly\n",
        "        'model': 'tiny',           # Smallest possible model\n",
        "        'skip_vad': True,          # Skip voice activity detection\n",
        "        'skip_align': True,        # Skip alignment\n",
        "        'skip_diarize': True,      # Skip diarization\n",
        "        'chunk_duration': 180      # 3-minute chunks\n",
        "    },\n",
        "    'minimal': {\n",
        "        'method': 'whisperx',\n",
        "        'model': 'base',\n",
        "        'skip_vad': True,\n",
        "        'skip_align': False,\n",
        "        'skip_diarize': True,\n",
        "        'chunk_duration': 300\n",
        "    },\n",
        "    'standard': {\n",
        "        'method': 'whisperx',\n",
        "        'model': 'small',\n",
        "        'skip_vad': False,\n",
        "        'skip_align': False,\n",
        "        'skip_diarize': True,\n",
        "        'chunk_duration': 600\n",
        "    },\n",
        "    'high': {\n",
        "        'method': 'whisperx',\n",
        "        'model': 'medium',\n",
        "        'skip_vad': False,\n",
        "        'skip_align': False,\n",
        "        'skip_diarize': False,\n",
        "        'chunk_duration': 900\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "RekgeJyUkFrf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.5: SAFE DEPENDENCY INSTALLATION\n",
        "# ============================================\n",
        "\n",
        "def ensure_dependencies(mode):\n",
        "    \"\"\"Install only necessary dependencies for current mode\"\"\"\n",
        "    log_message(f\"Checking dependencies for {mode} mode...\")\n",
        "\n",
        "    # Basic dependencies always needed\n",
        "    basic_deps = ['pydub']\n",
        "    for dep in basic_deps:\n",
        "        try:\n",
        "            __import__(dep)\n",
        "        except ImportError:\n",
        "            log_message(f\"Installing {dep}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', dep])\n",
        "\n",
        "    # FFmpeg\n",
        "    if subprocess.call(['which', 'ffmpeg'], stdout=subprocess.DEVNULL) != 0:\n",
        "        log_message(\"Installing ffmpeg...\")\n",
        "        subprocess.call(['apt-get', '-qq', 'update'])\n",
        "        subprocess.call(['apt-get', '-qq', 'install', 'ffmpeg'])\n",
        "\n",
        "    # Mode-specific dependencies\n",
        "    if PROCESSING_MODES[mode]['method'] == 'whisper_api':\n",
        "        try:\n",
        "            import whisper\n",
        "        except ImportError:\n",
        "            log_message(\"Installing OpenAI Whisper...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'openai-whisper'])\n",
        "\n",
        "    elif PROCESSING_MODES[mode]['method'] == 'whisperx':\n",
        "        try:\n",
        "            import whisperx\n",
        "        except ImportError:\n",
        "            log_message(\"Installing WhisperX...\")\n",
        "            # Install with specific order to avoid conflicts\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch==2.0.0', 'torchaudio==2.0.0', '--index-url', 'https://download.pytorch.org/whl/cu118'])\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'git+https://github.com/m-bain/whisperx.git'])\n"
      ],
      "metadata": {
        "id": "G9dDOG6JkFmm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.6: ULTRA-MINIMAL TRANSCRIPTION\n",
        "# ============================================\n",
        "\n",
        "def transcribe_ultra_minimal(audio_path):\n",
        "    \"\"\"Use OpenAI Whisper directly - most stable option\"\"\"\n",
        "    import whisper\n",
        "\n",
        "    log_message(\"Loading tiny Whisper model...\")\n",
        "    model = whisper.load_model(\"tiny\")\n",
        "\n",
        "    log_message(\"Transcribing with OpenAI Whisper...\")\n",
        "    result = model.transcribe(audio_path, language='en')\n",
        "\n",
        "    # Convert to WhisperX-like format\n",
        "    segments = []\n",
        "    if 'segments' in result:\n",
        "        for seg in result['segments']:\n",
        "            segments.append({\n",
        "                'start': seg['start'],\n",
        "                'end': seg['end'],\n",
        "                'text': seg['text']\n",
        "            })\n",
        "    else:\n",
        "        # Fallback if no segments\n",
        "        segments.append({\n",
        "            'start': 0,\n",
        "            'end': 0,\n",
        "            'text': result.get('text', '')\n",
        "        })\n",
        "\n",
        "    # Clean up model\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "    return {'segments': segments}\n"
      ],
      "metadata": {
        "id": "_lL5PDgfkFjc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.7: WHISPERX TRANSCRIPTION\n",
        "# ============================================\n",
        "\n",
        "def transcribe_whisperx(audio_path, mode_config):\n",
        "    \"\"\"Use WhisperX with configurable features\"\"\"\n",
        "    import whisperx\n",
        "    import torch\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load only necessary models\n",
        "    log_message(f\"Loading {mode_config['model']} model on {device}...\")\n",
        "\n",
        "    # Load with minimal configuration\n",
        "    model = whisperx.load_model(\n",
        "        mode_config['model'],\n",
        "        device,\n",
        "        compute_type=\"int8\",  # Always use int8 for stability\n",
        "        language='en',\n",
        "        asr_options={\n",
        "            \"suppress_numerals\": True,\n",
        "            \"max_new_tokens\": None,\n",
        "            \"clip_timestamps\": None,\n",
        "            \"hallucination_silence_threshold\": None,\n",
        "            \"hotwords\": None\n",
        "        } if mode_config['skip_vad'] else {}\n",
        "    )\n",
        "\n",
        "    # Load audio\n",
        "    audio = whisperx.load_audio(audio_path)\n",
        "\n",
        "    # Transcribe with minimal batch size\n",
        "    log_message(\"Transcribing...\")\n",
        "    result = model.transcribe(audio, batch_size=1)\n",
        "\n",
        "    # Optional alignment\n",
        "    if not mode_config['skip_align']:\n",
        "        try:\n",
        "            log_message(\"Aligning transcript...\")\n",
        "            model_a, metadata = whisperx.load_align_model(language_code='en', device=device)\n",
        "            result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device)\n",
        "            del model_a\n",
        "        except Exception as e:\n",
        "            log_message(f\"Alignment failed: {e}\", \"WARNING\")\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if device == \"cuda\" else None\n",
        "    gc.collect()\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "LtLHueNHjcXV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.8: AUDIO PROCESSING\n",
        "# ============================================\n",
        "\n",
        "def convert_to_wav(input_path, output_path):\n",
        "    \"\"\"Convert audio file to WAV format\"\"\"\n",
        "    try:\n",
        "        from pydub import AudioSegment\n",
        "        log_message(f\"Converting to WAV: {os.path.basename(input_path)}\")\n",
        "\n",
        "        audio = AudioSegment.from_file(input_path)\n",
        "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "\n",
        "        log_message(f\"Saved WAV to cache: {os.path.basename(output_path)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        log_message(f\"Conversion failed: {e}\", \"ERROR\")\n",
        "        return False\n",
        "\n",
        "def split_audio_for_processing(wav_path, chunk_duration):\n",
        "    \"\"\"Split audio into smaller chunks\"\"\"\n",
        "    from pydub import AudioSegment\n",
        "\n",
        "    audio = AudioSegment.from_wav(wav_path)\n",
        "    total_duration = len(audio) / 1000  # seconds\n",
        "\n",
        "    if total_duration <= chunk_duration:\n",
        "        return [(wav_path, 0)]  # No need to split\n",
        "\n",
        "    chunks = []\n",
        "    chunk_ms = chunk_duration * 1000\n",
        "\n",
        "    for i in range(0, len(audio), chunk_ms):\n",
        "        chunk = audio[i:i + chunk_ms]\n",
        "        chunk_path = wav_path.replace('.wav', f'_chunk_{i//1000}.wav')\n",
        "        chunk.export(chunk_path, format=\"wav\")\n",
        "        chunks.append((chunk_path, i/1000))\n",
        "\n",
        "    log_message(f\"Split into {len(chunks)} chunks of {chunk_duration}s each\")\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "ZpluRJWjjcVK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.9: MAIN PROCESSING FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def process_single_file(filepath, checkpoint):\n",
        "    \"\"\"Process a single file with current mode\"\"\"\n",
        "    mode = checkpoint['current_mode']\n",
        "    mode_config = PROCESSING_MODES[mode]\n",
        "    base_name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "\n",
        "    log_message(f\"\\n{'='*60}\")\n",
        "    log_message(f\"Processing: {os.path.basename(filepath)}\")\n",
        "    log_message(f\"Mode: {mode}\")\n",
        "    log_message(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Get or create WAV file\n",
        "        wav_path = os.path.join(WAV_DIR, f\"{base_name}.wav\")\n",
        "        if not os.path.exists(wav_path):\n",
        "            if not filepath.endswith('.wav'):\n",
        "                if not convert_to_wav(filepath, wav_path):\n",
        "                    raise Exception(\"Failed to convert to WAV\")\n",
        "            else:\n",
        "                import shutil\n",
        "                shutil.copy2(filepath, wav_path)\n",
        "\n",
        "        # Check file size and split if needed\n",
        "        file_size_mb = os.path.getsize(wav_path) / (1024*1024)\n",
        "        log_message(f\"File size: {file_size_mb:.1f} MB\")\n",
        "\n",
        "        chunks = split_audio_for_processing(wav_path, mode_config['chunk_duration'])\n",
        "        all_segments = []\n",
        "\n",
        "        # Process each chunk\n",
        "        for chunk_idx, (chunk_path, start_offset) in enumerate(chunks):\n",
        "            if len(chunks) > 1:\n",
        "                log_message(f\"Processing chunk {chunk_idx+1}/{len(chunks)}...\")\n",
        "\n",
        "            # Choose transcription method\n",
        "            if mode_config['method'] == 'whisper_api':\n",
        "                result = transcribe_ultra_minimal(chunk_path)\n",
        "            else:\n",
        "                result = transcribe_whisperx(chunk_path, mode_config)\n",
        "\n",
        "            # Adjust timestamps and collect segments\n",
        "            for segment in result.get('segments', []):\n",
        "                segment['start'] = segment.get('start', 0) + start_offset\n",
        "                segment['end'] = segment.get('end', 0) + start_offset\n",
        "                all_segments.append(segment)\n",
        "\n",
        "            # Clean up chunk if it's temporary\n",
        "            if chunk_path != wav_path:\n",
        "                os.remove(chunk_path)\n",
        "\n",
        "            # Force garbage collection after each chunk\n",
        "            gc.collect()\n",
        "            time.sleep(1)  # Brief pause\n",
        "\n",
        "        # Save transcript\n",
        "        output_path = os.path.join(OUTPUT_DIR, f\"{base_name}_transcript.txt\")\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Transcription of: {os.path.basename(filepath)}\\n\")\n",
        "            f.write(f\"# Processing mode: {mode}\\n\")\n",
        "            f.write(f\"# Processed on: {datetime.now().isoformat()}\\n\\n\")\n",
        "\n",
        "            for seg_idx, segment in enumerate(all_segments):\n",
        "                start = segment.get('start', 0)\n",
        "                end = segment.get('end', 0)\n",
        "                text = segment.get('text', '').strip()\n",
        "\n",
        "                if text:  # Only write non-empty segments\n",
        "                    f.write(f\"[{start:.2f}-{end:.2f}] {text}\\n\")\n",
        "\n",
        "        log_message(f\"SUCCESS: Saved transcript to {os.path.basename(output_path)}\")\n",
        "\n",
        "        # Update checkpoint\n",
        "        checkpoint['processed_files'].append(os.path.basename(filepath))\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_message(f\"FAILED: {e}\", \"ERROR\")\n",
        "\n",
        "        # Log failure\n",
        "        checkpoint['failed_files'][os.path.basename(filepath)] = {\n",
        "            'error': str(e),\n",
        "            'mode': mode,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "ogdsmWf-jcTH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.10: GET PENDING FILES\n",
        "# ============================================\n",
        "\n",
        "def get_pending_files(checkpoint):\n",
        "    \"\"\"Get list of files not yet processed\"\"\"\n",
        "    all_files = []\n",
        "    supported_formats = ['.mp4', '.mp3', '.wav', '.m4a', '.flac', '.ogg']\n",
        "\n",
        "    try:\n",
        "        for filename in os.listdir(SOURCE_DIR):\n",
        "            if os.path.splitext(filename)[1].lower() in supported_formats:\n",
        "                all_files.append(filename)\n",
        "    except Exception as e:\n",
        "        log_message(f\"Error reading source directory: {e}\", \"ERROR\")\n",
        "        return []\n",
        "\n",
        "    # Filter out already processed files\n",
        "    pending = []\n",
        "    for f in all_files:\n",
        "        if f not in checkpoint['processed_files']:\n",
        "            base_name = os.path.splitext(f)[0]\n",
        "            transcript_path = os.path.join(OUTPUT_DIR, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "            if not os.path.exists(transcript_path):\n",
        "                pending.append(f)\n",
        "            else:\n",
        "                # File was processed but not in checkpoint\n",
        "                checkpoint['processed_files'].append(f)\n",
        "                log_message(f\"Found existing transcript for {f}, updating checkpoint\")\n",
        "\n",
        "    return sorted(pending)  # Sort for consistent ordering\n"
      ],
      "metadata": {
        "id": "_mubZIgwjcRI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.11: MAIN EXECUTION\n",
        "# ============================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "            log_message(\"Google Drive mounted\")\n",
        "        else:\n",
        "            log_message(\"Google Drive already mounted\")\n",
        "    except Exception as e:\n",
        "        log_message(f\"Could not mount Drive: {e}\", \"WARNING\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = load_checkpoint()\n",
        "\n",
        "    log_message(\"\\n\" + \"=\"*60)\n",
        "    log_message(f\"SESSION #{checkpoint.get('session_count', 0) + 1} STARTING\")\n",
        "    log_message(f\"Progress: {len(checkpoint['processed_files'])} files completed\")\n",
        "    log_message(f\"Failed: {len(checkpoint['failed_files'])} files\")\n",
        "    log_message(f\"Current mode: {checkpoint['current_mode']}\")\n",
        "    log_message(\"=\"*60)\n",
        "\n",
        "    # Get pending files\n",
        "    pending_files = get_pending_files(checkpoint)\n",
        "    log_message(f\"Files remaining: {len(pending_files)}\")\n",
        "\n",
        "    if not pending_files:\n",
        "        log_message(\"\\nAll files processed in current mode!\")\n",
        "\n",
        "        # Check if we should upgrade mode\n",
        "        modes = list(PROCESSING_MODES.keys())\n",
        "        current_idx = modes.index(checkpoint['current_mode'])\n",
        "\n",
        "        if current_idx < len(modes) - 1:\n",
        "            next_mode = modes[current_idx + 1]\n",
        "            log_message(f\"\\nUpgrading to {next_mode} mode for better quality...\")\n",
        "            checkpoint['current_mode'] = next_mode\n",
        "            checkpoint['processed_files'] = []\n",
        "            checkpoint['failed_files'] = {}\n",
        "            save_checkpoint(checkpoint)\n",
        "            pending_files = get_pending_files(checkpoint)\n",
        "        else:\n",
        "            log_message(\"\\nALL PROCESSING COMPLETE!\")\n",
        "            return\n",
        "\n",
        "    # Ensure dependencies for current mode\n",
        "    ensure_dependencies(checkpoint['current_mode'])\n",
        "\n",
        "    # Process files one by one\n",
        "    processed_count = 0\n",
        "    max_files_per_session = 3  # Process fewer files per session for stability\n",
        "\n",
        "    for filename in pending_files[:max_files_per_session]:\n",
        "        filepath = os.path.join(SOURCE_DIR, filename)\n",
        "\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            log_message(f\"File not found: {filename}\", \"WARNING\")\n",
        "            continue\n",
        "\n",
        "        success = process_single_file(filepath, checkpoint)\n",
        "        processed_count += 1\n",
        "\n",
        "        # Longer cooldown between files\n",
        "        if processed_count < len(pending_files):\n",
        "            log_message(\"Cooling down for 15 seconds...\")\n",
        "            time.sleep(15)\n",
        "\n",
        "        # Force garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "    # Summary\n",
        "    log_message(\"\\n\" + \"=\"*60)\n",
        "    log_message(f\"SESSION COMPLETE\")\n",
        "    log_message(f\"Processed this session: {processed_count}\")\n",
        "    log_message(f\"Total completed: {len(checkpoint['processed_files'])}\")\n",
        "    log_message(f\"Still pending: {len(pending_files) - processed_count}\")\n",
        "    log_message(\"=\"*60)\n",
        "\n",
        "    if len(pending_files) > processed_count:\n",
        "        log_message(\"\\nRun this cell again to continue processing!\")\n",
        "\n",
        "    # Final cleanup\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "ZGG6juA3jcOu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.12: EXECUTE MAIN\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        log_message(\"\\nProcess interrupted by user\", \"WARNING\")\n",
        "    except Exception as e:\n",
        "        log_message(f\"\\nFATAL ERROR: {e}\", \"ERROR\")\n",
        "        import traceback\n",
        "        log_message(traceback.format_exc(), \"ERROR\")\n"
      ],
      "metadata": {
        "id": "-SgmBlWijcM1",
        "outputId": "8aed92cc-8fb7-4a4d-d6df-4deec6565b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-06-18 19:43:46] INFO: Google Drive already mounted\n",
            "[2025-06-18 19:43:46] INFO: \n",
            "============================================================\n",
            "[2025-06-18 19:43:46] INFO: SESSION #1 STARTING\n",
            "[2025-06-18 19:43:46] INFO: Progress: 0 files completed\n",
            "[2025-06-18 19:43:46] INFO: Failed: 0 files\n",
            "[2025-06-18 19:43:46] INFO: Current mode: ultra_minimal\n",
            "[2025-06-18 19:43:46] INFO: ============================================================\n",
            "[2025-06-18 19:43:46] ERROR: Error reading source directory: [Errno 2] No such file or directory: '/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT'\n",
            "[2025-06-18 19:43:46] INFO: Files remaining: 0\n",
            "[2025-06-18 19:43:46] INFO: \n",
            "All files processed in current mode!\n",
            "[2025-06-18 19:43:46] INFO: \n",
            "Upgrading to minimal mode for better quality...\n",
            "[2025-06-18 19:43:46] INFO: Checkpoint saved (session #1)\n",
            "[2025-06-18 19:43:46] ERROR: Error reading source directory: [Errno 2] No such file or directory: '/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT'\n",
            "[2025-06-18 19:43:46] INFO: Checking dependencies for minimal mode...\n",
            "[2025-06-18 19:43:46] INFO: Installing WhisperX...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CODE BLOCK 1.13: USAGE INSTRUCTIONS\n",
        "# ============================================\n",
        "\n",
        "\"\"\"\n",
        "CRASH-PROOF TRANSCRIPTION SYSTEM\n",
        "\n",
        "SETUP:\n",
        "1. GPU Runtime: Use T4 or CPU (system adapts automatically)\n",
        "2. Just run this single cell - it handles everything\n",
        "\n",
        "AFTER CRASH:\n",
        "1. Reconnect to runtime\n",
        "2. Run this same cell again\n",
        "3. It automatically resumes from checkpoint\n",
        "\n",
        "PROCESSING MODES (AUTOMATIC PROGRESSION):\n",
        "1. ultra_minimal: OpenAI Whisper tiny model (most stable)\n",
        "2. minimal: WhisperX base model, no VAD\n",
        "3. standard: WhisperX small model with VAD\n",
        "4. high: WhisperX medium model with diarization\n",
        "\n",
        "FEATURES:\n",
        "- Saves after EVERY file\n",
        "- Logs all operations\n",
        "- 3-minute chunks for stability\n",
        "- Automatic mode progression\n",
        "- Crash recovery built-in\n",
        "\n",
        "MONITORING:\n",
        "- Check progress: cat /content/drive/MyDrive/PRUT-Transcriptions/checkpoint.json\n",
        "- View logs: cat /content/drive/MyDrive/PRUT-Transcriptions/processing.log\n",
        "\n",
        "TROUBLESHOOTING:\n",
        "- If crashes persist, manually edit checkpoint.json:\n",
        "  \"current_mode\": \"ultra_minimal\"\n",
        "- Delete specific files from \"processed_files\" array to reprocess\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RDFV0dxGjcCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V7\n",
        "\n",
        "## First Time Setup:\n",
        "\n",
        "python# Just run the main cell - it will:\n",
        " 1. Mount Drive\n",
        " 2. Install dependencies (only first time)\n",
        " 3. Create all directories\n",
        " 4. Start processing with minimal quality\n",
        "\n",
        "## After a Crash:\n",
        "\n",
        "python# Simply run the same cell again!\n",
        " It will:\n",
        " 1. Read checkpoint.json\n",
        " 2. Skip already processed files\n",
        " 3. Continue from where it stopped"
      ],
      "metadata": {
        "id": "lQRnO9RmhCMp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5kI6DuF7gPSI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFK7sz31gPPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WhisperX Checkpoint-Based Transcription System\n",
        "# Designed to survive crashes and resume from last position\n",
        "\n",
        "# ============================================\n",
        "# SINGLE CELL - RUN THIS REPEATEDLY AFTER CRASHES\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "SOURCE_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/Transcripts'\n",
        "WAV_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/WAV_Cache'\n",
        "CHECKPOINT_FILE = '/content/drive/MyDrive/PRUT-Transcriptions/checkpoint.json'\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [OUTPUT_DIR, WAV_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "BznjQg6ggPT9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# CHECKPOINT MANAGEMENT\n",
        "# ============================================\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load progress from checkpoint file\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {\n",
        "        'processed_files': [],\n",
        "        'failed_files': {},\n",
        "        'current_quality': 'minimal',\n",
        "        'model_loaded': None,\n",
        "        'last_update': None\n",
        "    }\n",
        "\n",
        "def save_checkpoint(checkpoint):\n",
        "    \"\"\"Save progress to checkpoint file\"\"\"\n",
        "    checkpoint['last_update'] = datetime.now().isoformat()\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "    print(f\"✓ Checkpoint saved at {checkpoint['last_update']}\")\n",
        "\n",
        "def get_pending_files(checkpoint):\n",
        "    \"\"\"Get list of files not yet processed\"\"\"\n",
        "    all_files = []\n",
        "    supported_formats = ['.mp4', '.mp3', '.wav', '.m4a', '.flac', '.ogg']\n",
        "\n",
        "    for filename in os.listdir(SOURCE_DIR):\n",
        "        if os.path.splitext(filename)[1].lower() in supported_formats:\n",
        "            all_files.append(filename)\n",
        "\n",
        "    # Filter out already processed files\n",
        "    pending = [f for f in all_files if f not in checkpoint['processed_files']]\n",
        "\n",
        "    # Also check if transcript exists (in case checkpoint was corrupted)\n",
        "    verified_pending = []\n",
        "    for f in pending:\n",
        "        base_name = os.path.splitext(f)[0]\n",
        "        transcript_path = os.path.join(OUTPUT_DIR, f\"{base_name}_transcript.txt\")\n",
        "        if not os.path.exists(transcript_path):\n",
        "            verified_pending.append(f)\n",
        "        else:\n",
        "            # File was processed but not in checkpoint, update checkpoint\n",
        "            checkpoint['processed_files'].append(f)\n",
        "\n",
        "    return verified_pending\n"
      ],
      "metadata": {
        "id": "1qd69pJPgPNM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# QUALITY LEVELS CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "QUALITY_LEVELS = {\n",
        "    'minimal': {\n",
        "        'model': 'base',\n",
        "        'batch_size': 1,\n",
        "        'compute_type': 'int8',\n",
        "        'chunk_duration': 300,  # 5 minutes\n",
        "        'device': 'cuda',\n",
        "        'skip_diarization': True\n",
        "    },\n",
        "    'standard': {\n",
        "        'model': 'small',\n",
        "        'batch_size': 2,\n",
        "        'compute_type': 'float16',\n",
        "        'chunk_duration': 600,  # 10 minutes\n",
        "        'device': 'cuda',\n",
        "        'skip_diarization': True\n",
        "    },\n",
        "    'high': {\n",
        "        'model': 'medium',\n",
        "        'batch_size': 4,\n",
        "        'compute_type': 'float16',\n",
        "        'chunk_duration': 900,  # 15 minutes\n",
        "        'device': 'cuda',\n",
        "        'skip_diarization': False\n",
        "    },\n",
        "    'maximum': {\n",
        "        'model': 'large-v3',\n",
        "        'batch_size': 4,\n",
        "        'compute_type': 'float32',\n",
        "        'chunk_duration': 1800,  # 30 minutes (full file)\n",
        "        'device': 'cuda',\n",
        "        'skip_diarization': False\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "Mby75vl1gPLc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# DEPENDENCY INSTALLATION (ONLY IF NEEDED)\n",
        "# ============================================\n",
        "\n",
        "def ensure_dependencies():\n",
        "    \"\"\"Install dependencies only if not already installed\"\"\"\n",
        "    try:\n",
        "        import whisperx\n",
        "        print(\"✓ WhisperX already installed\")\n",
        "    except ImportError:\n",
        "        print(\"Installing WhisperX and dependencies...\")\n",
        "        os.system('pip install -q git+https://github.com/m-bain/whisperx.git')\n",
        "        os.system('pip install -q pydub')\n",
        "        print(\"✓ Dependencies installed\")\n",
        "\n",
        "    # Ensure ffmpeg\n",
        "    if os.system('which ffmpeg > /dev/null 2>&1') != 0:\n",
        "        print(\"Installing ffmpeg...\")\n",
        "        os.system('apt-get -qq update && apt-get -qq install ffmpeg')\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "Xn8Eg-hegPJB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# MODEL LOADING WITH MEMORY MANAGEMENT\n",
        "# ============================================\n",
        "\n",
        "def load_models_for_quality(quality_level):\n",
        "    \"\"\"Load models appropriate for quality level\"\"\"\n",
        "    config = QUALITY_LEVELS[quality_level]\n",
        "\n",
        "    # Clear any existing models first\n",
        "    if 'whisperx' in globals():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Import after ensuring dependencies\n",
        "    global whisperx\n",
        "    import whisperx\n",
        "\n",
        "    print(f\"\\nLoading {quality_level} quality models...\")\n",
        "    print(f\"Model: {config['model']}, Device: {config['device']}\")\n",
        "\n",
        "    try:\n",
        "        # Load main model\n",
        "        model = whisperx.load_model(\n",
        "            config['model'],\n",
        "            config['device'],\n",
        "            compute_type=config['compute_type'],\n",
        "            language='en'\n",
        "        )\n",
        "\n",
        "        # Load alignment model\n",
        "        model_a, metadata = whisperx.load_align_model(\n",
        "            language_code='en',\n",
        "            device=config['device']\n",
        "        )\n",
        "\n",
        "        # Diarization model (optional)\n",
        "        diarize_model = None\n",
        "        if not config['skip_diarization']:\n",
        "            try:\n",
        "                # Try to get HF token\n",
        "                from google.colab import userdata\n",
        "                HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "                # Import and load diarization\n",
        "                from pyannote.audio import Pipeline\n",
        "                diarize_model = Pipeline.from_pretrained(\n",
        "                    \"pyannote/speaker-diarization@2.1\",\n",
        "                    use_auth_token=HF_TOKEN\n",
        "                )\n",
        "                if config['device'] == 'cuda':\n",
        "                    diarize_model.to(torch.device('cuda'))\n",
        "                print(\"✓ Diarization model loaded\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Diarization not available: {e}\")\n",
        "                diarize_model = None\n",
        "\n",
        "        print(f\"✓ Models loaded for {quality_level} quality\")\n",
        "        return model, model_a, metadata, diarize_model, config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load {quality_level} models: {e}\")\n",
        "        if quality_level != 'minimal':\n",
        "            print(\"Falling back to minimal quality...\")\n",
        "            return load_models_for_quality('minimal')\n",
        "        else:\n",
        "            raise e\n"
      ],
      "metadata": {
        "id": "7Br2cJyfgPG5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# AUDIO PROCESSING FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def convert_to_wav(input_path, output_path):\n",
        "    \"\"\"Convert audio file to WAV format\"\"\"\n",
        "    try:\n",
        "        from pydub import AudioSegment\n",
        "        print(f\"Converting to WAV: {os.path.basename(input_path)}\")\n",
        "\n",
        "        audio = AudioSegment.from_file(input_path)\n",
        "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "\n",
        "        print(f\"✓ Saved WAV to cache: {os.path.basename(output_path)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Conversion failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def split_audio_file(wav_path, chunk_duration):\n",
        "    \"\"\"Split audio into chunks for processing\"\"\"\n",
        "    from pydub import AudioSegment\n",
        "\n",
        "    audio = AudioSegment.from_wav(wav_path)\n",
        "    chunks = []\n",
        "\n",
        "    # Calculate chunks\n",
        "    total_duration = len(audio)\n",
        "    chunk_ms = chunk_duration * 1000\n",
        "\n",
        "    for i in range(0, total_duration, chunk_ms):\n",
        "        chunk = audio[i:i + chunk_ms]\n",
        "        chunk_path = wav_path.replace('.wav', f'_chunk_{i//1000}.wav')\n",
        "        chunk.export(chunk_path, format=\"wav\")\n",
        "        chunks.append((chunk_path, i/1000))  # path and start time\n",
        "\n",
        "    print(f\"✓ Split into {len(chunks)} chunks of {chunk_duration}s each\")\n",
        "    return chunks\n",
        "\n",
        "def transcribe_chunk(chunk_path, model, model_a, metadata, config):\n",
        "    \"\"\"Transcribe a single audio chunk\"\"\"\n",
        "    audio = whisperx.load_audio(chunk_path)\n",
        "\n",
        "    # Transcribe with retry logic\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            result = model.transcribe(audio, batch_size=config['batch_size'])\n",
        "            break\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e) and attempt < 2:\n",
        "                print(f\"OOM - Retrying with smaller batch...\")\n",
        "                config['batch_size'] = max(1, config['batch_size'] // 2)\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    # Align\n",
        "    try:\n",
        "        result = whisperx.align(\n",
        "            result[\"segments\"], model_a, metadata,\n",
        "            audio, config['device'],\n",
        "            return_char_alignments=False\n",
        "        )\n",
        "    except:\n",
        "        print(\"⚠ Alignment failed, using unaligned segments\")\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "8VScVcUqgPE2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# MAIN PROCESSING FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def process_single_file(filepath, checkpoint, models):\n",
        "    \"\"\"Process a single file with checkpointing\"\"\"\n",
        "    model, model_a, metadata, diarize_model, config = models\n",
        "    base_name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {os.path.basename(filepath)}\")\n",
        "    print(f\"Quality: {checkpoint['current_quality']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Get or create WAV file\n",
        "        wav_path = os.path.join(WAV_DIR, f\"{base_name}.wav\")\n",
        "        if not os.path.exists(wav_path):\n",
        "            if filepath.endswith('.wav'):\n",
        "                # Copy WAV to cache\n",
        "                import shutil\n",
        "                shutil.copy2(filepath, wav_path)\n",
        "            else:\n",
        "                # Convert to WAV\n",
        "                if not convert_to_wav(filepath, wav_path):\n",
        "                    raise Exception(\"Failed to convert to WAV\")\n",
        "        else:\n",
        "            print(f\"✓ Using cached WAV file\")\n",
        "\n",
        "        # Step 2: Process in chunks if needed\n",
        "        file_size = os.path.getsize(wav_path) / (1024*1024)  # MB\n",
        "        print(f\"File size: {file_size:.1f} MB\")\n",
        "\n",
        "        if file_size > 50 and config['chunk_duration'] < 1800:\n",
        "            # Split large files\n",
        "            chunks = split_audio_file(wav_path, config['chunk_duration'])\n",
        "            all_segments = []\n",
        "\n",
        "            for chunk_idx, (chunk_path, start_offset) in enumerate(chunks):\n",
        "                print(f\"\\nProcessing chunk {chunk_idx+1}/{len(chunks)}...\")\n",
        "\n",
        "                result = transcribe_chunk(chunk_path, model, model_a, metadata, config)\n",
        "\n",
        "                # Adjust timestamps\n",
        "                for segment in result.get('segments', []):\n",
        "                    segment['start'] += start_offset\n",
        "                    segment['end'] += start_offset\n",
        "                    all_segments.append(segment)\n",
        "\n",
        "                # Clean up chunk\n",
        "                os.remove(chunk_path)\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            result = {'segments': all_segments}\n",
        "        else:\n",
        "            # Process whole file\n",
        "            result = transcribe_chunk(wav_path, model, model_a, metadata, config)\n",
        "\n",
        "        # Step 3: Save transcript\n",
        "        output_path = os.path.join(OUTPUT_DIR, f\"{base_name}_transcript.txt\")\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# Transcription of: {os.path.basename(filepath)}\\n\")\n",
        "            f.write(f\"# Quality level: {checkpoint['current_quality']}\\n\")\n",
        "            f.write(f\"# Processed on: {datetime.now().isoformat()}\\n\\n\")\n",
        "\n",
        "            for segment in result.get('segments', []):\n",
        "                start = segment.get('start', 0)\n",
        "                end = segment.get('end', 0)\n",
        "                text = segment.get('text', '').strip()\n",
        "                speaker = segment.get('speaker', 'Speaker1')\n",
        "\n",
        "                f.write(f\"{speaker} [{start:.2f}-{end:.2f}]: {text}\\n\")\n",
        "\n",
        "        print(f\"\\n✅ SUCCESS: Saved transcript to {output_path}\")\n",
        "\n",
        "        # Update checkpoint\n",
        "        checkpoint['processed_files'].append(os.path.basename(filepath))\n",
        "        if os.path.basename(filepath) in checkpoint['failed_files']:\n",
        "            del checkpoint['failed_files'][os.path.basename(filepath)]\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ FAILED: {e}\")\n",
        "\n",
        "        # Record failure\n",
        "        checkpoint['failed_files'][os.path.basename(filepath)] = {\n",
        "            'error': str(e),\n",
        "            'quality': checkpoint['current_quality'],\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "_UtX75kIgPCk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# MAIN EXECUTION LOOP\n",
        "# ============================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function - run this after each crash\"\"\"\n",
        "\n",
        "    # Mount Google Drive if needed\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"✓ Google Drive mounted\")\n",
        "    except:\n",
        "        print(\"⚠ Could not mount Drive - assuming already mounted\")\n",
        "\n",
        "    # Ensure dependencies\n",
        "    ensure_dependencies()\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = load_checkpoint()\n",
        "    print(f\"\\n📊 Progress Report:\")\n",
        "    print(f\"Files processed: {len(checkpoint['processed_files'])}\")\n",
        "    print(f\"Files failed: {len(checkpoint['failed_files'])}\")\n",
        "    print(f\"Current quality: {checkpoint['current_quality']}\")\n",
        "\n",
        "    # Get pending files\n",
        "    pending_files = get_pending_files(checkpoint)\n",
        "    print(f\"Files remaining: {len(pending_files)}\")\n",
        "\n",
        "    if not pending_files:\n",
        "        print(\"\\n✅ All files processed!\")\n",
        "\n",
        "        # Check if we should upgrade quality\n",
        "        qualities = list(QUALITY_LEVELS.keys())\n",
        "        current_idx = qualities.index(checkpoint['current_quality'])\n",
        "\n",
        "        if current_idx < len(qualities) - 1:\n",
        "            next_quality = qualities[current_idx + 1]\n",
        "            print(f\"\\n🔄 Upgrading to {next_quality} quality and reprocessing...\")\n",
        "            checkpoint['current_quality'] = next_quality\n",
        "            checkpoint['processed_files'] = []\n",
        "            save_checkpoint(checkpoint)\n",
        "            pending_files = get_pending_files(checkpoint)\n",
        "        else:\n",
        "            print(\"\\n🎉 All files processed at maximum quality!\")\n",
        "            return\n",
        "\n",
        "    # Load models for current quality\n",
        "    try:\n",
        "        models = load_models_for_quality(checkpoint['current_quality'])\n",
        "        checkpoint['model_loaded'] = checkpoint['current_quality']\n",
        "        save_checkpoint(checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load models: {e}\")\n",
        "        return\n",
        "\n",
        "    # Process files one by one\n",
        "    processed_count = 0\n",
        "    for filename in pending_files:\n",
        "        filepath = os.path.join(SOURCE_DIR, filename)\n",
        "\n",
        "        success = process_single_file(filepath, checkpoint, models)\n",
        "        processed_count += 1\n",
        "\n",
        "        # Cool down between files\n",
        "        if processed_count < len(pending_files):\n",
        "            print(\"\\n⏳ Cooling down for 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "        # Check if we've processed enough for this session\n",
        "        if processed_count >= 5:  # Process 5 files per run to avoid timeout\n",
        "            print(f\"\\n⚠ Processed {processed_count} files. Run again to continue.\")\n",
        "            break\n",
        "\n",
        "    # Final cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n📊 Session Summary:\")\n",
        "    print(f\"Files processed this session: {processed_count}\")\n",
        "    print(f\"Total files completed: {len(checkpoint['processed_files'])}\")\n",
        "    print(f\"Files remaining: {len(pending_files) - processed_count}\")\n",
        "    print(\"\\n💡 If runtime crashed, just run this cell again!\")\n"
      ],
      "metadata": {
        "id": "n7Rz0HH7gPAb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# RUN THE MAIN FUNCTION\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Q7VjiKiGgO-V",
        "outputId": "793b4cd4-7fd2-46e2-8392-73d2620957db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'main' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1678827910.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# USAGE INSTRUCTIONS\n",
        "# ============================================\n",
        "\"\"\"\n",
        "SETUP:\n",
        "1. Set runtime to GPU (T4 or better)\n",
        "2. Add HF_TOKEN to Colab secrets (optional, for diarization)\n",
        "3. Run this single cell\n",
        "\n",
        "AFTER CRASH:\n",
        "1. Reconnect to runtime\n",
        "2. Run this same cell again\n",
        "3. It will automatically resume from where it left off\n",
        "\n",
        "FEATURES:\n",
        "- Saves progress after each file\n",
        "- Caches WAV conversions\n",
        "- Starts with fast/minimal quality\n",
        "- Automatically upgrades quality after all files processed\n",
        "- Splits large files into chunks\n",
        "- Handles out-of-memory errors gracefully\n",
        "\n",
        "QUALITY PROGRESSION:\n",
        "1. Minimal: base model, no diarization (fastest)\n",
        "2. Standard: small model, better accuracy\n",
        "3. High: medium model with diarization\n",
        "4. Maximum: large-v3 model, best quality\n",
        "\n",
        "FILES ARE SAVED TO:\n",
        "- Transcripts: /PRUT-Transcriptions/Transcripts/\n",
        "- WAV Cache: /PRUT-Transcriptions/WAV_Cache/\n",
        "- Checkpoint: /PRUT-Transcriptions/checkpoint.json\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9dG0d6RagOvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V6"
      ],
      "metadata": {
        "id": "j8_a_XglgL5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WhisperX Transcription with Speaker Diarization\n",
        "# Updated for English transcription with MP4 support\n",
        "\n",
        "# ============================================\n",
        "# STEP 1: GPU Setup and Verification\n",
        "# ============================================\n",
        "# First, set Runtime to GPU (T4) in Colab: Runtime > Change runtime type > GPU\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Verify GPU availability\n",
        "tf_device = tf.test.gpu_device_name()\n",
        "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if tf_device != '/device:GPU:0' or torch_device.type != 'cuda':\n",
        "    raise SystemError('GPU not found. Please enable GPU in Runtime settings.')\n",
        "\n",
        "print(f'TensorFlow GPU: {tf_device}')\n",
        "print(f'PyTorch GPU: {torch_device}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "\n",
        "# Check GPU details\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "H6tNlAfYZzFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6d3881-cb51-4e89-e7c9-452ebd173dc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow GPU: /device:GPU:0\n",
            "PyTorch GPU: cuda\n",
            "CUDA available: True\n",
            "Wed Jun 18 19:01:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0             26W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 2: Install Dependencies\n",
        "# ============================================\n",
        "!pip install -q pydub\n",
        "!pip install -q git+https://github.com/m-bain/whisperx.git\n",
        "\n",
        "# Additional dependencies for video processing\n",
        "!apt-get -qq install ffmpeg\n"
      ],
      "metadata": {
        "id": "50X4kP-jZ09l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c10222-282c-4ec0-84ae-cad2aa1e5d2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whisperx (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 3: Import Libraries and Set Locale\n",
        "# ============================================\n",
        "import os\n",
        "import gc\n",
        "import locale\n",
        "from pydub import AudioSegment\n",
        "from google.colab import drive, userdata\n",
        "import whisperx\n",
        "\n",
        "# Set UTF-8 locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "os.environ['LC_ALL'] = 'C.UTF-8'\n",
        "os.environ['LANG'] = 'C.UTF-8'\n",
        "\n",
        "print(f\"Locale encoding: {locale.getpreferredencoding()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ENBuIEaqBt",
        "outputId": "87958812-4a2d-497c-8c04-475780e6269a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Locale encoding: utf-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 4: Mount Google Drive\n",
        "# ============================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update these paths to your actual directories\n",
        "SOURCE_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/Recordings_PRUT'  # UPDATE THIS\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/PRUT-Transcriptions/Transcripts'  # UPDATE THIS\n",
        "TEMP_AUDIO_DIR = '/content/temp_audio'  # Temporary directory for WAV files\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "WlIvq5-raAhr",
        "outputId": "a151a77e-0a8d-4a9a-804b-3664f0ed963a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'drive' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-612465585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# STEP 4: Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Update these paths to your actual directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 5: Configure WhisperX with Secrets\n",
        "# ============================================\n",
        "# Store your HuggingFace token in Colab secrets:\n",
        "# Click the key icon in the left sidebar > Add a secret named 'HF_TOKEN'\n",
        "\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    print(\"Warning: HF_TOKEN not found in secrets. Using hardcoded token.\")\n",
        "    HF_TOKEN = \"your_huggingface_token_here\"  # Fallback - replace with your token\n",
        "\n",
        "# WhisperX configuration\n",
        "device = \"cuda\"\n",
        "batch_size = 4  # Adjust based on GPU memory (start with 4, can try 6 or 8)\n",
        "compute_type = \"float32\"  # Options: \"float32\", \"float16\", \"int8\"\n",
        "language = \"en\"  # Changed from 'de' to 'en'\n"
      ],
      "metadata": {
        "id": "r1OfsB-8aBsb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 6: Load WhisperX Models\n",
        "# ============================================\n",
        "print(\"Loading WhisperX models...\")\n",
        "\n",
        "# Load main transcription model\n",
        "model = whisperx.load_model(\"large-v3\", device, language=language, compute_type=compute_type)\n",
        "\n",
        "# Load alignment model\n",
        "model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
        "\n",
        "# Load diarization model\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
        "\n",
        "print(\"All models loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "EPyhDPNOaC5h",
        "outputId": "7dc1d39a-9a93-425a-d116-13ca9ce7821d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WhisperX models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.11/dist-packages/whisperx/assets/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>Performing voice activity detection using Pyannote...\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
            "100%|██████████| 360M/360M [00:01<00:00, 275MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'whisperx' has no attribute 'DiarizationPipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1637757256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load diarization model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdiarize_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiarizationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHF_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All models loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'whisperx' has no attribute 'DiarizationPipeline'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 7: Audio Conversion Functions\n",
        "# ============================================\n",
        "def convert_to_wav(input_path, output_path):\n",
        "    \"\"\"Convert MP4/MP3/other formats to WAV\"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(input_path)\n",
        "        # Convert to mono, 16kHz for WhisperX\n",
        "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {input_path}: {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "9W0VnxiSaEEj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 8: Main Transcription Function\n",
        "# ============================================\n",
        "def transcribe_with_diarization(audio_path, min_speakers=1, max_speakers=10):\n",
        "    \"\"\"Transcribe audio with speaker diarization\"\"\"\n",
        "\n",
        "    # Load audio\n",
        "    audio = whisperx.load_audio(audio_path)\n",
        "\n",
        "    # Transcribe\n",
        "    print(f\"Transcribing {os.path.basename(audio_path)}...\")\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "\n",
        "    # Align whisper output\n",
        "    print(\"Aligning transcript...\")\n",
        "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device,\n",
        "                           return_char_alignments=False)\n",
        "\n",
        "    # Diarize\n",
        "    print(\"Performing speaker diarization...\")\n",
        "    diarize_segments = diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
        "\n",
        "    # Assign speakers to words\n",
        "    result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "-_3jL2YRaG0k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 9: Process All Files\n",
        "# ============================================\n",
        "# Define speaker mapping\n",
        "speaker_labels = {}\n",
        "speaker_counter = 1\n",
        "\n",
        "# Supported formats\n",
        "supported_formats = ['.mp4', '.mp3', '.wav', '.m4a', '.flac', '.ogg']\n",
        "\n",
        "# Process all files\n",
        "for filename in os.listdir(SOURCE_DIR):\n",
        "    file_ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "    if file_ext in supported_formats:\n",
        "        try:\n",
        "            input_path = os.path.join(SOURCE_DIR, filename)\n",
        "            base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Convert to WAV if needed\n",
        "            if file_ext != '.wav':\n",
        "                print(f\"\\nConverting {filename} to WAV...\")\n",
        "                wav_path = os.path.join(TEMP_AUDIO_DIR, f\"{base_name}.wav\")\n",
        "                if not convert_to_wav(input_path, wav_path):\n",
        "                    continue\n",
        "            else:\n",
        "                wav_path = input_path\n",
        "\n",
        "            # Transcribe with diarization\n",
        "            result = transcribe_with_diarization(wav_path)\n",
        "\n",
        "            # Map speakers to sequential labels\n",
        "            for segment in result[\"segments\"]:\n",
        "                if 'speaker' in segment and segment['speaker'] not in speaker_labels:\n",
        "                    speaker_labels[segment['speaker']] = f\"Speaker{speaker_counter}\"\n",
        "                    speaker_counter += 1\n",
        "\n",
        "            # Save transcription\n",
        "            output_path = os.path.join(OUTPUT_DIR, f\"{base_name}_transcript.txt\")\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                for segment in result[\"segments\"]:\n",
        "                    speaker = speaker_labels.get(segment.get('speaker', 'Unknown'), 'Unknown')\n",
        "                    start = segment['start']\n",
        "                    end = segment['end']\n",
        "                    text = segment['text']\n",
        "                    f.write(f\"{speaker} [{start:.2f}-{end:.2f}]: {text}\\n\")\n",
        "\n",
        "            print(f\"✓ Saved transcript to {output_path}\")\n",
        "\n",
        "            # Clean up temporary WAV file\n",
        "            if file_ext != '.wav' and os.path.exists(wav_path):\n",
        "                os.remove(wav_path)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(f\"⚠️  Out of memory for {filename}. Clearing cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                # Optionally reduce batch size\n",
        "                batch_size = max(1, batch_size - 1)\n",
        "                print(f\"Reduced batch size to {batch_size}\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"❌ Error processing {filename}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {filename}: {e}\")\n",
        "\n",
        "        # Clear GPU memory after each file\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "print(\"\\n✅ Transcription complete!\")\n",
        "print(f\"Processed files saved to: {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pUdJ7-z0aIa1",
        "outputId": "fb27932a-2e91-44a5-e3b8-aad2aee3fec5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3326615050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Process all files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mfile_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 10: Clean Up (Optional)\n",
        "# ============================================\n",
        "# Run this to free GPU memory when done\n",
        "del model, model_a, diarize_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"GPU memory cleared\")\n"
      ],
      "metadata": {
        "id": "OX79xHZaaJkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# OPTIONAL: Advanced Configuration\n",
        "# ============================================\n",
        "\"\"\"\n",
        "Advanced options you can modify:\n",
        "\n",
        "1. Batch Size:\n",
        "   - Start with 4\n",
        "   - Increase to 6 or 8 if GPU has enough memory\n",
        "   - Decrease to 2 or 1 if you get out-of-memory errors\n",
        "\n",
        "2. Compute Type:\n",
        "   - \"float32\": Best accuracy (default)\n",
        "   - \"float16\": Faster, slightly less accurate\n",
        "   - \"int8\": Fastest, least accurate\n",
        "\n",
        "3. Model Size:\n",
        "   - \"large-v3\": Best accuracy (current)\n",
        "   - \"medium\": Faster, good accuracy\n",
        "   - \"small\": Much faster, lower accuracy\n",
        "   - \"base\": Fastest, lowest accuracy\n",
        "\n",
        "4. Speaker Count:\n",
        "   - Adjust min_speakers and max_speakers based on your audio\n",
        "   - Set both to same number if you know exact speaker count\n",
        "\n",
        "5. Language:\n",
        "   - Change language parameter for other languages\n",
        "   - See WhisperX documentation for supported languages\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QhtGDy5VXIjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}