{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ8W7dZEMQ8elFOKEFdpeD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SingularitySmith/PRUT-Transcriber/blob/main/PRUT_Transcriber4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "hLUY6cd3QFIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WhisperX Transcription with Speaker Diarization\n",
        "# Updated for English transcription with MP4 support\n",
        "\n",
        "# ============================================\n",
        "# STEP 1: GPU Setup and Verification\n",
        "# ============================================\n",
        "# First, set Runtime to GPU (T4) in Colab: Runtime > Change runtime type > GPU\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Verify GPU availability\n",
        "tf_device = tf.test.gpu_device_name()\n",
        "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if tf_device != '/device:GPU:0' or torch_device.type != 'cuda':\n",
        "    raise SystemError('GPU not found. Please enable GPU in Runtime settings.')\n",
        "\n",
        "print(f'TensorFlow GPU: {tf_device}')\n",
        "print(f'PyTorch GPU: {torch_device}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "\n",
        "# Check GPU details\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "H6tNlAfYZzFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 2: Install Dependencies\n",
        "# ============================================\n",
        "!pip install -q pydub\n",
        "!pip install -q git+https://github.com/m-bain/whisperx.git\n",
        "\n",
        "# Additional dependencies for video processing\n",
        "!apt-get -qq install ffmpeg\n",
        "\n",
        "# ============================================\n",
        "# STEP 3: Import Libraries and Set Locale\n",
        "# ============================================\n",
        "import os\n",
        "import gc\n",
        "import locale\n",
        "from pydub import AudioSegment\n",
        "from google.colab import drive, userdata\n",
        "import whisperx\n",
        "\n",
        "# Set UTF-8 locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "os.environ['LC_ALL'] = 'C.UTF-8'\n",
        "os.environ['LANG'] = 'C.UTF-8'\n",
        "\n",
        "print(f\"Locale encoding: {locale.getpreferredencoding()}\")\n"
      ],
      "metadata": {
        "id": "50X4kP-jZ09l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 4: Mount Google Drive\n",
        "# ============================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update these paths to your actual directories\n",
        "SOURCE_DIR = '/content/drive/My Drive/YourFolder/InputVideos'  # UPDATE THIS\n",
        "OUTPUT_DIR = '/content/drive/My Drive/YourFolder/Transcriptions'  # UPDATE THIS\n",
        "TEMP_AUDIO_DIR = '/content/temp_audio'  # Temporary directory for WAV files\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "WlIvq5-raAhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 5: Configure WhisperX with Secrets\n",
        "# ============================================\n",
        "# Store your HuggingFace token in Colab secrets:\n",
        "# Click the key icon in the left sidebar > Add a secret named 'HF_TOKEN'\n",
        "\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    print(\"Warning: HF_TOKEN not found in secrets. Using hardcoded token.\")\n",
        "    HF_TOKEN = \"your_huggingface_token_here\"  # Fallback - replace with your token\n",
        "\n",
        "# WhisperX configuration\n",
        "device = \"cuda\"\n",
        "batch_size = 4  # Adjust based on GPU memory (start with 4, can try 6 or 8)\n",
        "compute_type = \"float32\"  # Options: \"float32\", \"float16\", \"int8\"\n",
        "language = \"en\"  # Changed from 'de' to 'en'\n"
      ],
      "metadata": {
        "id": "r1OfsB-8aBsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 6: Load WhisperX Models\n",
        "# ============================================\n",
        "print(\"Loading WhisperX models...\")\n",
        "\n",
        "# Load main transcription model\n",
        "model = whisperx.load_model(\"large-v3\", device, language=language, compute_type=compute_type)\n",
        "\n",
        "# Load alignment model\n",
        "model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
        "\n",
        "# Load diarization model\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
        "\n",
        "print(\"All models loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "EPyhDPNOaC5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 7: Audio Conversion Functions\n",
        "# ============================================\n",
        "def convert_to_wav(input_path, output_path):\n",
        "    \"\"\"Convert MP4/MP3/other formats to WAV\"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(input_path)\n",
        "        # Convert to mono, 16kHz for WhisperX\n",
        "        audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {input_path}: {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "9W0VnxiSaEEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# STEP 8: Main Transcription Function\n",
        "# ============================================\n",
        "def transcribe_with_diarization(audio_path, min_speakers=2, max_speakers=10):\n",
        "    \"\"\"Transcribe audio with speaker diarization\"\"\"\n",
        "\n",
        "    # Load audio\n",
        "    audio = whisperx.load_audio(audio_path)\n",
        "\n",
        "    # Transcribe\n",
        "    print(f\"Transcribing {os.path.basename(audio_path)}...\")\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "\n",
        "    # Align whisper output\n",
        "    print(\"Aligning transcript...\")\n",
        "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device,\n",
        "                           return_char_alignments=False)\n",
        "\n",
        "    # Diarize\n",
        "    print(\"Performing speaker diarization...\")\n",
        "    diarize_segments = diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
        "\n",
        "    # Assign speakers to words\n",
        "    result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "\n",
        "    return result\n",
        "\n",
        "# ============================================\n",
        "# STEP 9: Process All Files\n",
        "# ============================================\n",
        "# Define speaker mapping\n",
        "speaker_labels = {}\n",
        "speaker_counter = 1\n",
        "\n",
        "# Supported formats\n",
        "supported_formats = ['.mp4', '.mp3', '.wav', '.m4a', '.flac', '.ogg']\n",
        "\n",
        "# Process all files\n",
        "for filename in os.listdir(SOURCE_DIR):\n",
        "    file_ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "    if file_ext in supported_formats:\n",
        "        try:\n",
        "            input_path = os.path.join(SOURCE_DIR, filename)\n",
        "            base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Convert to WAV if needed\n",
        "            if file_ext != '.wav':\n",
        "                print(f\"\\nConverting {filename} to WAV...\")\n",
        "                wav_path = os.path.join(TEMP_AUDIO_DIR, f\"{base_name}.wav\")\n",
        "                if not convert_to_wav(input_path, wav_path):\n",
        "                    continue\n",
        "            else:\n",
        "                wav_path = input_path\n",
        "\n",
        "            # Transcribe with diarization\n",
        "            result = transcribe_with_diarization(wav_path)\n",
        "\n",
        "            # Map speakers to sequential labels\n",
        "            for segment in result[\"segments\"]:\n",
        "                if 'speaker' in segment and segment['speaker'] not in speaker_labels:\n",
        "                    speaker_labels[segment['speaker']] = f\"Speaker{speaker_counter}\"\n",
        "                    speaker_counter += 1\n",
        "\n",
        "            # Save transcription\n",
        "            output_path = os.path.join(OUTPUT_DIR, f\"{base_name}_transcript.txt\")\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                for segment in result[\"segments\"]:\n",
        "                    speaker = speaker_labels.get(segment.get('speaker', 'Unknown'), 'Unknown')\n",
        "                    start = segment['start']\n",
        "                    end = segment['end']\n",
        "                    text = segment['text']\n",
        "                    f.write(f\"{speaker} [{start:.2f}-{end:.2f}]: {text}\\n\")\n",
        "\n",
        "            print(f\"✓ Saved transcript to {output_path}\")\n",
        "\n",
        "            # Clean up temporary WAV file\n",
        "            if file_ext != '.wav' and os.path.exists(wav_path):\n",
        "                os.remove(wav_path)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(f\"⚠️  Out of memory for {filename}. Clearing cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                # Optionally reduce batch size\n",
        "                batch_size = max(1, batch_size - 1)\n",
        "                print(f\"Reduced batch size to {batch_size}\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"❌ Error processing {filename}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {filename}: {e}\")\n",
        "\n",
        "        # Clear GPU memory after each file\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "print(\"\\n✅ Transcription complete!\")\n",
        "print(f\"Processed files saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 10: Clean Up (Optional)\n",
        "# ============================================\n",
        "# Run this to free GPU memory when done\n",
        "del model, model_a, diarize_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"GPU memory cleared\")\n",
        "\n",
        "# ============================================\n",
        "# OPTIONAL: Advanced Configuration\n",
        "# ============================================\n",
        "\"\"\"\n",
        "Advanced options you can modify:\n",
        "\n",
        "1. Batch Size:\n",
        "   - Start with 4\n",
        "   - Increase to 6 or 8 if GPU has enough memory\n",
        "   - Decrease to 2 or 1 if you get out-of-memory errors\n",
        "\n",
        "2. Compute Type:\n",
        "   - \"float32\": Best accuracy (default)\n",
        "   - \"float16\": Faster, slightly less accurate\n",
        "   - \"int8\": Fastest, least accurate\n",
        "\n",
        "3. Model Size:\n",
        "   - \"large-v3\": Best accuracy (current)\n",
        "   - \"medium\": Faster, good accuracy\n",
        "   - \"small\": Much faster, lower accuracy\n",
        "   - \"base\": Fastest, lowest accuracy\n",
        "\n",
        "4. Speaker Count:\n",
        "   - Adjust min_speakers and max_speakers based on your audio\n",
        "   - Set both to same number if you know exact speaker count\n",
        "\n",
        "5. Language:\n",
        "   - Change language parameter for other languages\n",
        "   - See WhisperX documentation for supported languages\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QhtGDy5VXIjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}